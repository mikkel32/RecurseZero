# RecurseZero

<p align="center">
  <strong>ğŸ§  GPU-Resident Chess RL Agent with Deep Equilibrium Models</strong>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#training">Training</a> â€¢
  <a href="#api">API</a>
</p>

---

## Overview

RecurseZero is a novel chess AI that achieves superhuman performance on consumer hardware by:

- **Eliminating CPU bottleneck**: All computation runs on GPU via JAX-native Pgx
- **O(1) memory depth**: Deep Equilibrium Models provide infinite effective depth
- **Search-free inference**: Muesli algorithm learns instinctive play without MCTS

## Features

| Feature | Description | Spec Reference |
|---------|-------------|----------------|
| ğŸš€ GPU-Resident | Game logic + neural net on GPU | Section 1.2 |
| â™¾ï¸ DEQ Core | Infinite depth, fixed memory | Section 2.2 |
| âš¡ Anderson Acceleration | Fast fixed-point convergence | Section 2.2.1 |
| ğŸ”’ GTrXL Gating | Stable recursive dynamics | Section 2.3 |
| â™Ÿï¸ Chess Position Bias | 2D relative encodings | Section 2.4 |
| ğŸ¯ Muesli Algorithm | Search-free policy | Section 3.1 |
| ğŸ“Š PVE Learning | Value-equivalent representations | Section 3.3 |
| ğŸ”¢ Int8 Quantization | 2-4x speedup via AQT | Section 4.1 |
| ğŸ“š Distillation | Teacher-student learning | Section 4.2 |

## Quick Start

### Google Colab (Recommended)

```python
# Install dependencies
!pip install jax[cuda12] flax optax pgx rich aqtp

# Clone repository
!git clone https://github.com/your-repo/RecurseZero.git
%cd RecurseZero

# Run training
!python main.py
```

### Local Setup

```bash
# Clone
git clone https://github.com/your-repo/RecurseZero.git
cd RecurseZero

# Setup environment
./setup.sh

# Run
python main.py
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RecurseZero Agent                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input (8Ã—8Ã—119)                                            â”‚
â”‚       â†“                                                     â”‚
â”‚  Dense Embedding â†’ (B, 64, hidden_dim)                      â”‚
â”‚       â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Deep Equilibrium Model (DEQ)               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚     Universal Transformer Block             â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ ChessformerAttention + Position Bias    â”‚â†â”€â”€â”€â”¤   â”‚
â”‚  â”‚  â”‚  â€¢ GTrXL Gating (stability)                 â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ QuantizedMLP (Int8)                      â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚         â†‘                                           â”‚   â”‚
â”‚  â”‚         â””â”€â”€ Anderson Acceleration (fixed-point) â”€â”€â”€â”˜   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â†“                                                     â”‚
â”‚  Mean Pooling â†’ (B, hidden_dim)                             â”‚
â”‚       â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ Policy  â”‚  â”‚  Value  â”‚  â”‚  Reward  â”‚                    â”‚
â”‚  â”‚ (4672)  â”‚  â”‚ [-1,1]  â”‚  â”‚  scalar  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Training

### Configuration

| Parameter | Default (Fast) | Full Spec | Description |
|-----------|----------------|-----------|-------------|
| `hidden_dim` | 128 | 256 | Embedding dimension |
| `heads` | 4 | 8 | Attention heads |
| `mlp_dim` | 512 | 1024 | MLP hidden size |
| `deq_iters` | 4 | 8 | DEQ iterations |
| `batch_size` | 2048 | 4096 | Parallel games |

### Metrics

The training loop displays:

- **P(win)**: Win probability from value head (spec 5.2)
- **Conf**: Policy confidence (inverse of entropy)
- **Games**: Total games completed (W/L/D)
- **s/s**: Training steps per second

### Expected Output

```
Step   100 â”‚ Loss: 0.0012 â”‚ P(win): 51.2% â”‚ Conf: 45% â”‚ Games: 1,024 (W:512/L:512/D:0) â”‚ 1.8 s/s
```

## API

### Training

```python
from training.loop import resident_train_step, create_train_state
from model.agent import RecurseZeroAgentFast

agent = RecurseZeroAgentFast(num_actions=4672)
state = create_train_state(agent, params)
state, env_state, metrics = resident_train_step(state, env_state, key, agent.apply, env.step, env._init)
```

### Distillation

```python
from training.distillation import DistillationTrainer, DistillationConfig

config = DistillationConfig(temperature=2.0, alpha=0.7)
trainer = DistillationTrainer(teacher_model, student_model, config)
trainer.load_teacher("teacher.pkl")
```

### Win Probability

```python
from algorithm.pve import value_to_win_probability

value = agent.apply(params, obs)[1]  # Get value head output
win_prob = value_to_win_probability(value)  # Convert to P(win)
```

## File Structure

```
RecurseZero/
â”œâ”€â”€ main.py                 # Entry point with training loop
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ agent.py            # RecurseZeroAgent (full & fast variants)
â”‚   â”œâ”€â”€ deq.py              # Deep Equilibrium Model + Anderson Accel
â”‚   â”œâ”€â”€ universal_transformer.py  # Transformer block with quantization
â”‚   â”œâ”€â”€ gtrxl.py            # GTrXL gating mechanism
â”‚   â””â”€â”€ embeddings.py       # Chess relative position bias
â”œâ”€â”€ algorithm/
â”‚   â”œâ”€â”€ muesli.py           # Muesli policy gradient
â”‚   â””â”€â”€ pve.py              # Proper Value Equivalence
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ loop.py             # GPU-resident training step
â”‚   â””â”€â”€ distillation.py     # Teacher-student distillation
â”œâ”€â”€ env/
â”‚   â””â”€â”€ pgx_wrapper.py      # GPU-resident Pgx chess
â””â”€â”€ optimization/
    â”œâ”€â”€ quantization.py     # Int8 AQT integration
    â””â”€â”€ hardware_compat.py  # MPS/CUDA configuration
```

## References

Based on the "GPU ONLY Chess RL Agent" specification:

1. **Pgx**: Hardware-accelerated game simulators (arXiv:2303.17503)
2. **DEQ**: Deep Equilibrium Models (arXiv:1909.01377)
3. **GTrXL**: Stabilizing Transformers for RL (arXiv:1910.06764)
4. **Muesli**: Search-free policy optimization (arXiv:2104.06159)
5. **AQT**: Accurate Quantized Training (Google Cloud)

## License

MIT License - See LICENSE file for details.
